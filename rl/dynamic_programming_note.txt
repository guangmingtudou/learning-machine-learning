基于动态学习的强化学习算法主要两种: 策略迭代和价值迭代

策略迭代(policy iteration): 策略评估(policy evaluation)和策略提升(policy improvement)

策略评估使用贝尔曼期望方程来得到一个策略的状态价值函数, 而价值迭代直接使用贝尔曼最优方程来进行动态规划
基于动态规划的强化学习算法要求事先知道环境的状态转移函数和奖励函数, 且只适用于有限马尔可夫决策过程

策略提升定理: 如果存在一个确定性的策略pi', 在任意一个状态s下, 都满足 Qpi(s,pi'(s))>=Vpi(s), 则Vpi'(s)>=Vpi(s)
贪心策略: pi'(s) = argmaxQpi(s,a) = argmax{r(s,a)+gamma(epsilon(P(s'|s,a)Vpi(s')))}

策略迭代算法: 随机初始化策略pi(s)和价值函数V(s), 